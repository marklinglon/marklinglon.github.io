{"meta":{"title":"汪茫人海","subtitle":"","description":"","author":"mark long","url":"https://marklinglon.github.io","root":"/"},"pages":[{"title":"404 Not Found：该页无法显示","date":"2023-05-22T16:03:51.536Z","updated":"2023-05-22T16:03:51.536Z","comments":false,"path":"/404.html","permalink":"https://marklinglon.github.io/404.html","excerpt":"","text":""},{"title":"关于","date":"2023-05-22T16:03:51.537Z","updated":"2023-05-22T16:03:51.537Z","comments":false,"path":"about/index.html","permalink":"https://marklinglon.github.io/about/index.html","excerpt":"","text":"个人详细介绍"},{"title":"书单","date":"2023-05-22T16:03:51.537Z","updated":"2023-05-22T16:03:51.537Z","comments":false,"path":"books/index.html","permalink":"https://marklinglon.github.io/books/index.html","excerpt":"","text":""},{"title":"分类","date":"2023-05-22T16:03:51.537Z","updated":"2023-05-22T16:03:51.537Z","comments":false,"path":"categories/index.html","permalink":"https://marklinglon.github.io/categories/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2023-05-22T16:03:51.537Z","updated":"2023-05-22T16:03:51.537Z","comments":false,"path":"repository/index.html","permalink":"https://marklinglon.github.io/repository/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2023-05-22T16:03:51.537Z","updated":"2023-05-22T16:03:51.537Z","comments":true,"path":"links/index.html","permalink":"https://marklinglon.github.io/links/index.html","excerpt":"","text":""},{"title":"标签","date":"2023-05-22T16:03:51.538Z","updated":"2023-05-22T16:03:51.538Z","comments":false,"path":"tags/index.html","permalink":"https://marklinglon.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"wechat gpt","slug":"wechatgpt","date":"2023-05-20T16:00:00.000Z","updated":"2023-05-26T05:53:15.898Z","comments":true,"path":"2023/05/21/wechatgpt/","link":"","permalink":"https://marklinglon.github.io/2023/05/21/wechatgpt/","excerpt":"","text":"下载 git clone https://github.com/869413421/wechatbot.git 进入项目目录 cd wechatbot 创建配置文件 123456cat &gt;&gt; config.json &lt;&lt;EOF&#123; &quot;api_key&quot;: &quot;your gpt api key&quot;, &quot;auto_pass&quot;: true&#125;EOF 启动项目 go run main.go 换账号登陆 rm -f storage.json go run main.go","categories":[{"name":"gpt","slug":"gpt","permalink":"https://marklinglon.github.io/categories/gpt/"}],"tags":[{"name":"gpt","slug":"gpt","permalink":"https://marklinglon.github.io/tags/gpt/"}]},{"title":"jumpserver问题处理","slug":"jumpserver","date":"2023-05-20T16:00:00.000Z","updated":"2023-05-29T03:44:35.295Z","comments":true,"path":"2023/05/21/jumpserver/","link":"","permalink":"https://marklinglon.github.io/2023/05/21/jumpserver/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. 登陆提示密码过期 WARNING: Your password has expired. You must change your password now and login again! Changing password for user zeo.yang. Current password: 处理 管理员登陆jumpserver 修改用户密码 用securecrt登陆该用户，发现问题依旧 chage -l 用户名 // 查看用户密码过期时间 修改宿主机的密码策略 vim /etc/login.def PASS_MAX_DAYS 99999 ansible TestCvm -m shell -a “chage -M -1 username” // 设置某个用户的密码过期时间永不过期","categories":[{"name":"jumpserver","slug":"jumpserver","permalink":"https://marklinglon.github.io/categories/jumpserver/"}],"tags":[{"name":"jumpserer","slug":"jumpserer","permalink":"https://marklinglon.github.io/tags/jumpserer/"}]},{"title":"K8s文章分享","slug":"share","date":"2022-03-20T16:00:00.000Z","updated":"2023-05-26T05:57:20.013Z","comments":true,"path":"2022/03/21/share/","link":"","permalink":"https://marklinglon.github.io/2022/03/21/share/","excerpt":"","text":"分享链接 minikube minikube部署k8s集群 https://mp.weixin.qq.com/s/xHpmEZM7-kjWar_z4ADWXg kubeadmin kubeadmin部署k8s集群 https://mp.weixin.qq.com/s/WvIg6uszw9QIkIT05cYcgQ kind kind部署k8s集群 https://mp.weixin.qq.com/s/YG5dNGH-T75HsXmLOQh5AA rke rke部署k8s集群 https://mp.weixin.qq.com/s/A4CJei7plYE9PJ2W2RRAOw ceph k8s私有云分布式存储搭建 https://mp.weixin.qq.com/s/CdLioTzU4oWI688lqYKXUQ ceph-k8s k8s对接ceph https://mp.weixin.qq.com/s/lHEC83E1iKy7ojUadWNU6w k8s k8s自建集群 https://mp.weixin.qq.com/s/473vdYANq2E_R51Lh94-9Q k8s-multus-CNI K8s Multus CNI的部署与工作原理 https://mp.weixin.qq.com/s/oSxR3ex2mnLk0qJdD6WGJg k8s-cni k8s自建cni https://mp.weixin.qq.com/s/K6ynL_9nSTLCTy0_2xCobg k8s-operator operator最佳实践 http://blazehu.com/2022/04/10/cloudnative/kubebuilder/ argocd/flux Argo CD 与 Flux CD — Kubernetes 集群的正确 GitOps 工具 https://mp.weixin.qq.com/s/RVmt6INalZdsGAxwRX_Veg prometheus 云原生大型分布式监控系统(一): 大规模场景下 Prometheus 的优化手段 https://mp.weixin.qq.com/s/Pd1ip05z8zxVKaAPmuKNnw prometheus 云原生大型分布式监控系统(二): Thanos 架构详解 https://mp.weixin.qq.com/s/oGyrJ4QiQ9KSYLuMSJnYYQ prometheus 云原生大型分布式监控系统(三): Thanos 部署与实践 https://mp.weixin.qq.com/s/sinLteFNKGNI1-vBv28xGg prometheus 云原生大型分布式监控系统(四): Kvass+Thanos 监控超大规模容器集群 https://mp.weixin.qq.com/s/gjW21wium2ZxVSBKE-hHtQ kubecm Kubeconfig文件自动合并-实现K8S多集群切换 https://mp.weixin.qq.com/s/2f2cAWMd03AdOt2QJEapPA kluster-capacity K8s 集群容量 - kluster capacity https://mp.weixin.qq.com/s/6VEut9TR8Y0Y6VuurjAarw k8s-scheduler K8s 调度系统由浅入深 https://mp.weixin.qq.com/s/fizeaWjrtZD-EwuVQIt3ag Karmada K8s 多集群管理 – Karmada 调度器 https://mp.weixin.qq.com/s/OdRMAPxV1lPGhsKivSYH_Q mimirtool prometheus瘦身工具 https://mp.weixin.qq.com/s/z23gYsLIkvbBePg-FUMJXA Kubernetes Descheduler k8s二次调度 https://mp.weixin.qq.com/s/kfqyRgagvHWeOiTprFqH9w","categories":[{"name":"k8s","slug":"k8s","permalink":"https://marklinglon.github.io/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://marklinglon.github.io/tags/k8s/"}]},{"title":"hexo部署文档","slug":"hexo","date":"2021-01-20T16:00:00.000Z","updated":"2023-05-26T05:51:36.805Z","comments":true,"path":"2021/01/21/hexo/","link":"","permalink":"https://marklinglon.github.io/2021/01/21/hexo/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start Create a new post 1$ hexo new &quot;My New Post&quot; More info: Writing Run server 1$ hexo server More info: Server Generate static files 1$ hexo generate More info: Generating Deploy to remote sites 12# deploy前先删除public目录$ hexo deploy 参考链接 123https://blog.cofess.com/2017/11/01/hexo-blog-theme-pure-usage-description.html // 部署文档http://blog.iwwee.com/posts/hexo-optimize.html // 优化https://hexo.io/zh-cn/docs/syntax-highlight.html // 代码高亮 More info: Deployment","categories":[{"name":"hexo","slug":"hexo","permalink":"https://marklinglon.github.io/categories/hexo/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://marklinglon.github.io/tags/hexo/"}]},{"title":"Kubebuilder Webhook 开发之创建 TLS 证书","slug":"kubebuilder-webhook","date":"2020-06-20T16:00:00.000Z","updated":"2023-05-26T05:53:07.967Z","comments":true,"path":"2020/06/21/kubebuilder-webhook/","link":"","permalink":"https://marklinglon.github.io/2020/06/21/kubebuilder-webhook/","excerpt":"","text":"在编写一个准入 Webhook 服务时，需要配置相关证书，k8s 提供了 api 用于对用户自主创建的证书进行认证签发。以下部分演示为 Webhook 服务创建 TLS 证书。 创建 TLS 证书 创建你的证书 通过运行以下命令生成私钥: 123456789101112131415cat &lt;&lt;EOF | cfssl genkey - | cfssljson -bare server&#123; &quot;hosts&quot;: [ &quot;my-svc.my-namespace.svc.cluster.local&quot;, &quot;my-pod.my-namespace.pod.cluster.local&quot;, &quot;192.0.2.24&quot;, &quot;10.0.34.2&quot; ], &quot;CN&quot;: &quot;my-pod.my-namespace.pod.cluster.local&quot;, &quot;key&quot;: &#123; &quot;algo&quot;: &quot;ecdsa&quot;, &quot;size&quot;: 256 &#125;&#125;EOF 此命令生成两个文件；它生成包含 PEM 编码 PKCS#10 证书请求的 server.csr， 以及 PEM 编码密钥的 server-key.pem，用于待生成的证书。 创建证书签名请求（CSR） 123456789101112cat &lt;&lt;EOF | kubectl apply -f -apiVersion: certificates.k8s.io/v1beta1kind: CertificateSigningRequestmetadata: name: examplespec: request: $(cat server.csr | base64 | tr -d &#x27;\\n&#x27;) usages: - digital signature - key encipherment - server authEOF 你能看到的输出类似于： 1certificatesigningrequest.certificates.k8s.io/example created Warning: certificates.k8s.io/v1beta1 CertificateSigningRequest is deprecated in v1.19+, unavailable in v1.22+; use certificates.k8s.io/v1 CertificateSigningRequest CSR 处于 Pending 状态。执行下面的命令你将可以看到： 1kubectl get csr 12NAME AGE SIGNERNAME REQUESTOR CONDITIONexample 17s kubernetes.io/legacy-unknown 100015926370-1650441195 Pending 批准证书签名请求（CSR） 1kubectl certificate approve example 1certificatesigningrequest.certificates.k8s.io/example approved 你现在应该能看到如下输出： 1kubectl get csr 12NAME AGE SIGNERNAME REQUESTOR CONDITIONexample 5m4s kubernetes.io/legacy-unknown 100015926370-1650441195 Approved,Issued 下载证书并使用它 1kubectl get csr example -o jsonpath=&#x27;&#123;.status.certificate&#125;&#x27; | base64 --decode &gt; server.crt 现在你可以将 server.crt 和 server-key.pem 作为你的服务的 https 认证了。 例如 kubebuilder 中使用 TLS 证书，将 server.crt 和 server-key.pem 放在 cert 目录中并修改名称为 tls.crt 和 tls.key，然后指定证书目录： 123456789mgr, err := ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options&#123; Scheme: scheme, MetricsBindAddress: metricsAddr, Port: 9443, HealthProbeBindAddress: probeAddr, LeaderElection: enableLeaderElection, LeaderElectionID: &quot;27e1b0af.blazehu.com&quot;, CertDir: &quot;./cert/&quot;,&#125;) 从 v1beta1 迁移到 v1 上述例子使用 certificates.k8s.io/v1beta1 API 版本的 CertificateSigningRequest 不在 v1.22 版本中继续提供。官方迁移指南点这里。 我们可以使用 certificates.k8s.io/v1 API 版本，此 API 从 v1.19 版本开始可用。 •certificates.k8s.io/v1 中需要额外注意的变更： •对于请求证书的 API 客户端而言： •spec.signerName 现在变成必需字段（参阅 已知的 Kubernetes 签署者）， 并且通过 certificates.k8s.io/v1 API 不可以创建签署者为 kubernetes.io/legacy-unknown 的请求 •spec.usages 现在变成必需字段，其中不可以包含重复的字符串值， 并且只能包含已知的用法字符串 创建你的证书 通过运行以下命令生成私钥: 123456789101112131415cat &lt;&lt;EOF | cfssl genkey - | cfssljson -bare server&#123; &quot;hosts&quot;: [ &quot;my-svc.my-namespace.svc.cluster.local&quot;, &quot;my-pod.my-namespace.pod.cluster.local&quot;, &quot;192.0.2.24&quot;, &quot;10.0.34.2&quot; ], &quot;CN&quot;: &quot;my-pod.my-namespace.pod.cluster.local&quot;, &quot;key&quot;: &#123; &quot;algo&quot;: &quot;ecdsa&quot;, &quot;size&quot;: 256 &#125;&#125;EOF 创建证书签名请求（CSR） 这里 csr signerName 不能是 kubernetes.io/legacy-unknown，演示我们随便指定一个为 example.com/serving，v1beta1 版本默认是 kubernetes.io/legacy-unknown。 12345678910111213cat &lt;&lt;EOF | kubectl apply -f -apiVersion: certificates.k8s.io/v1kind: CertificateSigningRequestmetadata: name: examplespec: request: $(cat server.csr | base64 | tr -d &#x27;\\n&#x27;) signerName: example.com/serving usages: - digital signature - key encipherment - server authEOF 批准证书签名请求（CSR） 1kubectl certificate approve example 1certificatesigningrequest.certificates.k8s.io/example approved 你现在应该能看到如下输出： 1kubectl get csr 12NAME AGE SIGNERNAME REQUESTOR CONDITIONexample 11s example.com/serving 100015926370-1650441195 Approved 这里可以看到证书请求已被批准，但是没有自动签名，正在等待请求的签名者对其签名。 签名证书签名请求（CSR） 我们扮演证书签署者的角色，颁发证书并将其上传到 API 服务器。 创建证书颁发机构 通过运行以下命令创建签名证书: 123456789cat &lt;&lt;EOF | cfssl gencert -initca - | cfssljson -bare ca&#123; &quot;CN&quot;: &quot;example.com/serving&quot;, &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;&#125;EOF 这会产生一个证书颁发机构密钥文件（ca-key.pem）和证书（ca.pem）。 颁发证书 创建文件 server-signing-config.json 内容如下： 123456789101112131415&#123; &quot;signing&quot;: &#123; &quot;default&quot;: &#123; &quot;usages&quot;: [ &quot;digital signature&quot;, &quot;key encipherment&quot;, &quot;server auth&quot; ], &quot;expiry&quot;: &quot;876000h&quot;, &quot;ca_constraint&quot;: &#123; &quot;is_ca&quot;: false &#125; &#125; &#125;&#125; 使用 server-signing-config.json 签名配置、证书颁发机构密钥文件和证书来签署证书请求： 1234kubectl get csr example -o jsonpath=&#x27;&#123;.spec.request&#125;&#x27; | \\base64 --decode | \\cfssl sign -ca ca.pem -ca-key ca-key.pem -config server-signing-config.json - | \\cfssljson -bare ca-signed-server 这会生成一个签名的服务证书文件，ca-signed-server.pem。 上传签名证书 123kubectl get csr example -o json | \\jq &#x27;.status.certificate = &quot;&#x27;$(base64 ca-signed-server.pem | tr -d &#x27;\\n&#x27;)&#x27;&quot;&#x27; | \\kubectl replace --raw /apis/certificates.k8s.io/v1/certificatesigningrequests/example/status -f - 批准 CSR 并上传签名证书后，你现在应该能看到如下输出： 1kubectl get csr 12NAME AGE SIGNERNAME REQUESTOR CONDITIONexample 10m example.com/serving 100015926370-1650441195 Approved,Issued 这是你可以正常下载证书并使用它了。 参考文档 •https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/certificate-signing-requests/ •https://kubernetes.io/zh-cn/docs/tasks/tls/managing-tls-in-a-cluster/#configuring-your-cluster-to-provide-signing •https://kubernetes.io/zh-cn/docs/reference/using-api/deprecation-guide/","categories":[{"name":"Kubebuilder","slug":"Kubebuilder","permalink":"https://marklinglon.github.io/categories/Kubebuilder/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://marklinglon.github.io/tags/k8s/"}]},{"title":"Kubebuilder Best Practices","slug":"kubebuilder-best-practices","date":"2020-06-20T16:00:00.000Z","updated":"2023-05-26T05:53:01.733Z","comments":true,"path":"2020/06/21/kubebuilder-best-practices/","link":"","permalink":"https://marklinglon.github.io/2020/06/21/kubebuilder-best-practices/","excerpt":"","text":"Kubebuilder is a framework for building Kubernetes APIs using custom resource definitions (CRDs). Note: kubebuilder can save us a lot of work and make developing CRDs and adminsion webhooks incredibly easy. Installation 123# download kubebuilder and install locally.curl -L -o kubebuilder https://go.kubebuilder.io/dl/latest/$(go env GOOS)/$(go env GOARCH)chmod +x kubebuilder &amp;&amp; mv kubebuilder /usr/local/bin/ Create a Project Create a directory, and then run the init command inside of it to initialize a new project. Follows an example. 1234567891011[marklu@MacBook ~]$ mkdir ~/Project/workspace-go/example[marklu@MacBook ~]$ cd ~/Project/workspace-go/example[marklu@MacBook ~]$ kubebuilder init --domain marklu.com --owner &quot;marklu&quot; --repo marklu.com/exampleWriting kustomize manifests for you to edit...Writing scaffold for you to edit...Get controller runtime:$ go get sigs.k8s.io/controller-runtime@v0.10.0Update dependencies:$ go mod tidyNext: define a resource with:$ kubebuilder create api If your project is initialized within GOPATH, the implicitly called go mod init will interpolate the module path for you. Otherwise –repo=must be set. Adding a new API 12345678910111213141516171819[marklu@MacBook ~]$ kubebuilder create api --group cos --version v1 --kind BucketCreate Resource [y/n]yCreate Controller [y/n]yWriting kustomize manifests for you to edit...Writing scaffold for you to edit...api/v1/bucket_types.gocontrollers/bucket_controller.goUpdate dependencies:$ go mod tidyRunning make:$ make generatego: creating new go.mod: module tmpDownloading sigs.k8s.io/controller-tools/cmd/controller-gen@v0.7.0go get: added sigs.k8s.io/controller-tools v0.7.0/Users/huyuhan/Project/workspace-go/example/bin/controller-gen object:headerFile=&quot;hack/boilerplate.go.txt&quot; paths=&quot;./...&quot;Next: implement your new API and generate the manifests (e.g. CRDs,CRs) with:$ make manifests Designing an API api/v1/bucket_types.go 12345678910// BucketSpec defines the desired state of Buckettype BucketSpec struct &#123; // INSERT ADDITIONAL SPEC FIELDS - desired state of cluster // Important: Run &quot;make&quot; to regenerate code after modifying this file // Foo is an example field of Bucket. Edit bucket_types.go to remove/update Name string `json:&quot;name,omitempty&quot;` Region string `json:&quot;region,omitempty&quot;` ACL string `json:&quot;acl,omitempty&quot;`&#125; Implementing a controller controllers/cos.go 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package controllersimport ( &quot;context&quot; &quot;fmt&quot; &quot;github.com/tencentyun/cos-go-sdk-v5&quot; &quot;net/http&quot; &quot;net/url&quot;)type CosStorage struct &#123; client *cos.Client accessKeyId string accessKeySecret string bucket string region string&#125;// NewCosStorage endpoint: https://cloud.tencent.com/document/product/436/6224func NewCosStorage(region, bucketName string) *CosStorage &#123; url, _ := url.Parse(fmt.Sprintf(&quot;https://%s.cos.%s.myqcloud.com&quot;, bucketName, region)) accessKeyId := &quot;&quot; accessKeySecret := &quot;&quot; b := &amp;cos.BaseURL&#123;BucketURL: url&#125; client := cos.NewClient(b, &amp;http.Client&#123; Transport: &amp;cos.AuthorizationTransport&#123; SecretID: accessKeyId, SecretKey: accessKeySecret, &#125;, &#125;) return &amp;CosStorage&#123; client: client, accessKeyId: accessKeyId, accessKeySecret: accessKeySecret, region: region, bucket: bucketName, &#125;&#125;func (c *CosStorage) Put(acl string) error &#123; opt := &amp;cos.BucketPutOptions&#123; XCosACL: acl, &#125; _, err := c.client.Bucket.Put(context.Background(), opt) return err&#125;func (c *CosStorage) Delete() error &#123; _, err := c.client.Bucket.Delete(context.Background()) return err&#125; controllers/bucket_controller.go tips: Finalizers allow controllers to implement asynchronous pre-delete hooks. Let’s say you create an external resource (such as a storage bucket) for each object of your API type, and you want to delete the associated external resource on object’s deletion from Kubernetes, you can use a finalizer to do that. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120/*Copyright 2022 marklu.Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.*/package controllersimport ( &quot;context&quot; &quot;k8s.io/apimachinery/pkg/runtime&quot; ctrl &quot;sigs.k8s.io/controller-runtime&quot; &quot;sigs.k8s.io/controller-runtime/pkg/client&quot; &quot;sigs.k8s.io/controller-runtime/pkg/controller/controllerutil&quot; &quot;sigs.k8s.io/controller-runtime/pkg/log&quot; cosv1 &quot;marklu.com/example/api/v1&quot;)// BucketReconciler reconciles a Bucket objecttype BucketReconciler struct &#123; client.Client Scheme *runtime.Scheme&#125;const ( bucketFinalizerName = &quot;bucket.cos.marklu.com/finalizer&quot;)//+kubebuilder:rbac:groups=cos.marklu.com,resources=buckets,verbs=get;list;watch;create;update;patch;delete//+kubebuilder:rbac:groups=cos.marklu.com,resources=buckets/status,verbs=get;update;patch//+kubebuilder:rbac:groups=cos.marklu.com,resources=buckets/finalizers,verbs=update// Reconcile is part of the main kubernetes reconciliation loop which aims to// move the current state of the cluster closer to the desired state.// TODO(user): Modify the Reconcile function to compare the state specified by// the Bucket object against the actual cluster state, and then// perform operations to make the cluster state reflect the state specified by// the user.//// For more details, check Reconcile and its Result here:// - https://pkg.go.dev/sigs.k8s.io/controller-runtime@v0.10.0/pkg/reconcilefunc (r *BucketReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) &#123; logger := log.FromContext(ctx) bucket := &amp;cosv1.Bucket&#123;&#125; if err := r.Get(ctx, req.NamespacedName, bucket); err != nil &#123; return ctrl.Result&#123;&#125;, client.IgnoreNotFound(err) &#125; // examine DeletionTimestamp to determine if object is under deletion if bucket.ObjectMeta.DeletionTimestamp.IsZero() &#123; // The object is not being deleted, so if it does not have our finalizer, // then lets add the finalizer and update the object. This is equivalent // registering our finalizer. if !controllerutil.ContainsFinalizer(bucket, bucketFinalizerName) &#123; controllerutil.AddFinalizer(bucket, bucketFinalizerName) if err := r.Update(ctx, bucket); err != nil &#123; return ctrl.Result&#123;&#125;, err &#125; &#125; else &#123; if err := r.updateExternalResources(bucket); err != nil &#123; logger.Error(err, &quot;unable to create Bucket&quot;) return ctrl.Result&#123;&#125;, err &#125; logger.Info(&quot;create Bucket succeed&quot;) &#125; &#125; else &#123; // The object is being deleted if controllerutil.ContainsFinalizer(bucket, bucketFinalizerName) &#123; // our finalizer is present, so lets handle any external dependency if err := r.deleteExternalResources(bucket); err != nil &#123; // if fail to delete the external dependency here, return with error // so that it can be retried logger.Error(err, &quot;unable to delete Bucket&quot;) return ctrl.Result&#123;&#125;, err &#125; // remove our finalizer from the list and update it. controllerutil.RemoveFinalizer(bucket, bucketFinalizerName) if err := r.Update(ctx, bucket); err != nil &#123; return ctrl.Result&#123;&#125;, err &#125; logger.Info(&quot;delete Bucket succeed&quot;) &#125; // Stop reconciliation as the item is being deleted return ctrl.Result&#123;&#125;, nil &#125; // bucket reconcile logic return ctrl.Result&#123;&#125;, nil&#125;func (r *BucketReconciler) updateExternalResources(bucket *cosv1.Bucket) error &#123; cosClient := NewCosStorage(bucket.Spec.Region, bucket.Spec.Name) return cosClient.Put(bucket.Spec.ACL)&#125;func (r *BucketReconciler) deleteExternalResources(bucket *cosv1.Bucket) error &#123; cosClient := NewCosStorage(bucket.Spec.Region, bucket.Spec.Name) return cosClient.Delete()&#125;// SetupWithManager sets up the controller with the Manager.func (r *BucketReconciler) SetupWithManager(mgr ctrl.Manager) error &#123; return ctrl.NewControllerManagedBy(mgr). For(&amp;cosv1.Bucket&#123;&#125;). Complete(r)&#125; Test It Out Install the CRDs into the cluster (make install) 1234[marklu@MacBook ~]$ make install/Users/huyuhan/Project/workspace-go/example/bin/controller-gen rbac:roleName=manager-role crd webhook paths=&quot;./...&quot; output:crd:artifacts:config=config/crd/bases/Users/huyuhan/Project/workspace-go/example/bin/kustomize build config/crd | kubectl apply -f -customresourcedefinition.apiextensions.k8s.io/buckets.cos.marklu.com created Run your controller (this will run in the foreground, so switch to a new terminal if you want to leave it running) (make run) 123456789101112[marklu@MacBook ~]$ make run/Users/huyuhan/Project/workspace-go/example/bin/controller-gen rbac:roleName=manager-role crd webhook paths=&quot;./...&quot; output:crd:artifacts:config=config/crd/bases/Users/huyuhan/Project/workspace-go/example/bin/controller-gen object:headerFile=&quot;hack/boilerplate.go.txt&quot; paths=&quot;./...&quot;go fmt ./...go vet ./...go run ./main.go2022-01-27T22:05:30.207+0800 INFO controller-runtime.metrics metrics server is starting to listen &#123;&quot;addr&quot;: &quot;:8080&quot;&#125;2022-01-27T22:05:30.207+0800 INFO setup starting manager2022-01-27T22:05:30.208+0800 INFO starting metrics server &#123;&quot;path&quot;: &quot;/metrics&quot;&#125;2022-01-27T22:05:30.208+0800 INFO controller.bucket Starting EventSource &#123;&quot;reconciler group&quot;: &quot;cos.marklu.com&quot;, &quot;reconciler kind&quot;: &quot;Bucket&quot;, &quot;source&quot;: &quot;kind source: /, Kind=&quot;&#125;2022-01-27T22:05:30.208+0800 INFO controller.bucket Starting Controller &#123;&quot;reconciler group&quot;: &quot;cos.marklu.com&quot;, &quot;reconciler kind&quot;: &quot;Bucket&quot;&#125;2022-01-27T22:05:30.309+0800 INFO controller.bucket Starting workers &#123;&quot;reconciler group&quot;: &quot;cos.marklu.com&quot;, &quot;reconciler kind&quot;: &quot;Bucket&quot;, &quot;worker count&quot;: 1&#125; Create Custom Resources (create bucket.cos.marklu.com/bucket-sample) (cos_v1_bucket.yaml) 12345678910apiVersion: cos.marklu.com/v1kind: Bucketmetadata: name: bucket-sample namespace: markluspec: # TODO(user): Add fields here name: example-1251762279 region: ap-shanghai acl: private kubectl apply -f cos_v1_bucket.yaml 12345[marklu@MacBook ~]$ kubectl apply -f cos_v1_bucket.yamlbucket.cos.marklu.com/bucket-sample created[marklu@MacBook ~]$ kubectl get bucket.cos.marklu.com -n markluNAME AGEbucket-sample 17s Tencent cloud console view found that the bucket was created normally. Delete Instances of Custom Resources (delete bucket.cos.marklu.com/bucket-sample) 12[marklu@MacBook ~]$ kubectl delete -f cos_v1_bucket.yamlbucket.cos.marklu.com &quot;bucket-sample&quot; deleted Run It On the Cluster Deploy the controller to the cluster with image specified by IMG 12make docker-build docker-push IMG=&lt;some-registry&gt;/&lt;project-name&gt;:tagmake deploy IMG=&lt;some-registry&gt;/&lt;project-name&gt;:tag Reference documentation https://github.com/kubernetes-sigs/kubebuilder https://book.kubebuilder.io/introduction.html https://kubernetes.io/docs/concepts/extend-kubernetes/operator/","categories":[{"name":"Kubebuilder","slug":"Kubebuilder","permalink":"https://marklinglon.github.io/categories/Kubebuilder/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://marklinglon.github.io/tags/k8s/"}]},{"title":"Kubebuilder Watch Rresources","slug":"kubebuilder-watch-resources","date":"2020-06-20T16:00:00.000Z","updated":"2023-05-26T05:53:05.309Z","comments":true,"path":"2020/06/21/kubebuilder-watch-resources/","link":"","permalink":"https://marklinglon.github.io/2020/06/21/kubebuilder-watch-resources/","excerpt":"","text":"我们在开发过程中，可能需要开发一个类似Deployment的资源逻辑，管理依赖资源是控制器的基础，如果不能观察它们的状态变化就不可能管理它们。这就意味着，我们需要 reconciler 能监控多个资源的变化。 NOTE: Deployment 必须知道其管理的 ReplicaSet 何时更改，ReplicaSet 必须知道其管理的 Pod 何时被删除，或者从健康变为不健康等。 控制器运行时库为管理和监视资源提供了多种方式。这包括从简单而明显的用例（例如查看由控制器创建和管理的资源）到更独特和更高级的用例。 •控制器创建和管理的资源 (Watching Operator Managed Resources) •外部管理的资源 (Watching Externally Managed Resources) 背景 以 Tcaplus 资源为例，Tcaplus 资源通过 ConfigMap（proto 文件）来创建表格。当 ConfigMap 发生变化时自动更新表格，下面例子不实际调用腾讯云API，只要验证接收到事件请求即可。 NOTE: TcaplusDB 是腾讯出品的分布式NoSQL数据库。官方API文档：https://cloud.tencent.com/document/product/596/39648。 控制器创建和管理的资源 资源定义 (Defined Tcaplus Resources) api/v1/tcaplus_types.go 123456789type TcaplusSpec struct &#123; Checksum string `json:&quot;checksum,omitempty&quot;` ConfigMapTemplate ConfigMapTemplate `json:&quot;configMapTemplate,omitempty&quot;`&#125;type ConfigMapTemplate struct &#123; Name string `json:&quot;name,omitempty&quot;` Data map[string]string `json:&quot;data,omitempty&quot;`&#125; 控制器逻辑 (Manage the Owned Resource) controllers/tcaplus_controller.go 当 tcaplus CR 创建时根据 ConfigMapTemplate 创建附属的 ConfigMap 资源并设置属主关系。 •Reconcile 方法：根据模版创建 ConfigMap 并设置属主关系 •SetupWithManager 方法：For 方法之后调用 Owns 方法 1234567891011121314151617181920212223242526272829303132333435func (r *TcaplusReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) &#123; logger := log.FromContext(ctx) logger.Info(&quot;reconciling&quot;) tcaplus := &amp;examplev1.Tcaplus&#123;&#125; if err := r.Get(ctx, req.NamespacedName, tcaplus); err != nil &#123; return ctrl.Result&#123;&#125;, client.IgnoreNotFound(err) &#125; configMap := &amp;corev1.ConfigMap&#123;&#125; configMap.Name = tcaplus.Spec.ConfigMapTemplate.Name configMap.Namespace = tcaplus.Namespace configMap.Data = tcaplus.Spec.ConfigMapTemplate.Data if err := controllerutil.SetControllerReference(tcaplus, configMap, r.Scheme); err != nil &#123; logger.Error(err, &quot;get configmap failed&quot;, &quot;configmap&quot;, configMap.Name) return ctrl.Result&#123;&#125;, err &#125; foundConfigMap := &amp;corev1.ConfigMap&#123;&#125; err := r.Get(ctx, types.NamespacedName&#123;Name: configMap.Name, Namespace: tcaplus.Namespace&#125;, foundConfigMap) if err != nil &amp;&amp; errors.IsNotFound(err) &#123; logger.V(1).Info(&quot;creating configmap&quot;, &quot;configmap&quot;, configMap.Name) err = r.Create(ctx, configMap) &#125; return ctrl.Result&#123;&#125;, nil&#125;// SetupWithManager sets up the controller with the Manager.func (r *TcaplusReconciler) SetupWithManager(mgr ctrl.Manager) error &#123; return ctrl.NewControllerManagedBy(mgr). For(&amp;examplev1.Tcaplus&#123;&#125;). Owns(&amp;corev1.ConfigMap&#123;&#125;). Complete(r)&#125; NOTE：同一控制器创建的资源才可以设置属主关系，不然会提示：already owned by another controller。 测试 config/samples/example_v1_tcaplus.yaml 1234567891011121314151617apiVersion: example.blazehu.com/v1kind: Tcaplusmetadata: name: tcaplus-samplespec: checksum: &quot;123&quot; configMapTemplate: name: &quot;tcaplus-configmap-example&quot; data: demo.proto: | syntax = &quot;proto3&quot;; package example; message Example &#123; uint32 a = 1; uint32 b = 2; uint32 c = 3; &#125; 使用上述配置文件创建 tcaplus 资源。创建结果： 123456marklu-MB2:samples $ k get tcaplusNAME AGEtcaplus-sample 19mmarklu-MB2:samples $ k get configmapNAME DATA AGEtcaplus-configmap-example 1 19m 可以查看 tcaplus-configmap-example 的属主关系： 123456789101112131415161718192021222324apiVersion: v1data: demo.proto: | syntax = &quot;proto3&quot;; package example; message Example &#123; uint32 a = 1; uint32 b = 2; &#125;kind: ConfigMapmetadata: creationTimestamp: &quot;2022-07-07T09:02:43Z&quot; name: tcaplus-configmap-example namespace: default ownerReferences: - apiVersion: example.blazehu.com/v1 blockOwnerDeletion: true controller: true kind: Tcaplus name: tcaplus-sample uid: 7c50f2e1-0e37-4aa0-bf49-c2d410d6153e resourceVersion: &quot;6837330713&quot; selfLink: /api/v1/namespaces/default/configmaps/tcaplus-configmap-example uid: 6c29f90b-0e51-4d9f-a6a8-cfb6906ed1b0 手动修改 tcaplus-sample 和 tcaplus-configmap-example 后查看控制器日志发现能正常观察 CR 和 ConfigMap 的变化了。 外部管理的资源 资源定义 (Defined Tcaplus Resources) api/v1/tcaplus_types.go 12345678type TcaplusSpec struct &#123; Checksum string `json:&quot;checksum,omitempty&quot;` ConfigMapRef ConfigMapReference `json:&quot;configMapRef,omitempty&quot;`&#125;type ConfigMapReference struct &#123; Name string `json:&quot;name,omitempty&quot;`&#125; 控制器逻辑 (Manage the Owned Resource) controllers/tcaplus_controller.go For 方法之后调用 Watches 方法就可以监听对应资源的事件，但是会监听集群里所有相关资源的事件，所以这里我们自定义事件处理方法来过滤出我们关注的资源的事件。 •通过 EnqueueRequestsFromMapFunc 创建一个事件处理方法，该方法通过 FieldSelector 在 ConfigMap 的事件中过滤出跟 tcaplus CR 相关联的事件。 •使用 FieldSelector 时我们需要建立对应的索引，使用 mgr.GetFieldIndexer().IndexField() 创建。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546const ( ConfigMapField = &quot;.spec.configMapRef.name&quot;) func (r *TcaplusReconciler) findObjectsForConfigMap(configMap client.Object) []reconcile.Request &#123; attachedTcaplusList := &amp;examplev1.TcaplusList&#123;&#125; listOps := &amp;client.ListOptions&#123; FieldSelector: fields.OneTermEqualSelector(ConfigMapField, configMap.GetName()), Namespace: configMap.GetNamespace(), &#125; err := r.List(context.TODO(), attachedTcaplusList, listOps) if err != nil &#123; return []reconcile.Request&#123;&#125; &#125; requests := make([]reconcile.Request, len(attachedTcaplusList.Items)) for i, item := range attachedTcaplusList.Items &#123; requests[i] = reconcile.Request&#123; NamespacedName: types.NamespacedName&#123; Name: item.GetName(), Namespace: item.GetNamespace(), &#125;, &#125; &#125; return requests&#125; // SetupWithManager sets up the controller with the Manager.func (r *TcaplusReconciler) SetupWithManager(mgr ctrl.Manager) error &#123; if err := mgr.GetFieldIndexer().IndexField(context.Background(), &amp;examplev1.Tcaplus&#123;&#125;, ConfigMapField, func(rawObj client.Object) []string &#123; tcaplus := rawObj.(*examplev1.Tcaplus) if tcaplus.Spec.ConfigMapRef.Name == &quot;&quot; &#123; return nil &#125; return []string&#123;tcaplus.Spec.ConfigMapRef.Name&#125; &#125;); err != nil &#123; return err &#125; return ctrl.NewControllerManagedBy(mgr). For(&amp;examplev1.Tcaplus&#123;&#125;). Watches( &amp;source.Kind&#123;Type: &amp;corev1.ConfigMap&#123;&#125;&#125;, handler.EnqueueRequestsFromMapFunc(r.findObjectsForConfigMap), builder.WithPredicates(predicate.ResourceVersionChangedPredicate&#123;&#125;), ). Complete(r)&#125; NOTE: 我们也可以自己定一个变更过滤器 Predicate。也可以通过 WithEventFilter 来针对监听的所有资源过滤。 测试 config/samples/example_v1_tcaplus.yaml 1234567891011121314151617181920212223apiVersion: v1kind: ConfigMapmetadata: name: tcaplus-configmap-exampledata: demo.proto: | syntax = &quot;proto3&quot;; package example; message Example &#123; uint32 a = 1; uint32 b = 2; uint32 c = 3; &#125;---apiVersion: example.blazehu.com/v1kind: Tcaplusmetadata: name: tcaplus-samplespec: checksum: &quot;123&quot; configMapRef: name: &quot;tcaplus-configmap-example&quot; 使用上述配置创建完毕后，手动修改 tcaplus-sample 和 tcaplus-configmap-example 查看控制器日志发现同样能正常观察 CR 和 ConfigMap 的变化。 NOTE: 查看 tcaplus-configmap-example 可以看到没有和 tcaplus 的属主关系。 总结 •EventHandler 可以在 watch 特定资源时设置该资源的事件监听规则。 •WithEventFilter 配置变更过滤器，可以针对 watch 的所有资源，统一地设置事件监听规则。 •Owns 源码分析可以发现 Owns 相当于调用 Watches(&amp;source.Kind{Type: }, &amp;handler.EnqueueRequestForOwner{OwnerType: apiType, IsController: true})。 参考文档 •https://www.kubebuilder.io/reference/watching-resources.html •https://kubernetes.io/zh-cn/docs/concepts/overview/working-with-objects/owners-dependents/ •https://segmentfault.com/a/1190000020359577","categories":[{"name":"Kubebuilder","slug":"Kubebuilder","permalink":"https://marklinglon.github.io/categories/Kubebuilder/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://marklinglon.github.io/tags/k8s/"}]},{"title":"Kubebuilder Admission Webhooks","slug":"kubebuilder-Admission-Webhooks","date":"2020-06-20T16:00:00.000Z","updated":"2023-05-26T05:52:57.691Z","comments":true,"path":"2020/06/21/kubebuilder-Admission-Webhooks/","link":"","permalink":"https://marklinglon.github.io/2020/06/21/kubebuilder-Admission-Webhooks/","excerpt":"","text":"什么是准入控制? 准入控制（Admission Controller）是 Kubernetes API Server 用于拦截请求的一种手段。Admission 可以做到对请求的资源对象进行校验，修改。service mesh 最近很火的项目 Istio 天生支持 Kubernetes，利用的就是 Admission 对服务实例自动注入 sidecar。 什么是准入 Webhook？ 准入 Webhook 是一种用于接收准入请求并对其进行处理的 HTTP 回调机制。 可以定义两种类型的准入 webhook，即 验证性质的准入 Webhook 和 修改性质的准入 Webhook。修改性质的准入 Webhook 会先被调用。它们可以更改发送到 API 服务器的对象以执行自定义的设置默认值操作。 在完成了所有对象修改并且 API 服务器也验证了所传入的对象之后， 验证性质的 Webhook 会被调用，并通过拒绝请求的方式来强制实施自定义的策略。 说明： 如果准入 Webhook 需要保证它们所看到的是对象的最终状态以实施某种策略。 则应使用验证性质的准入 Webhook，因为对象被修改性质 Webhook 看到之后仍然可能被修改。 尝试准入 Webhook 先决条件 •确保 Kubernetes 集群版本至少为 v1.16（以便使用 admissionregistration.k8s.io/v1 API） 或者 v1.9 （以便使 admissionregistration.k8s.io/v1beta1 API）。 •确保启用 MutatingAdmissionWebhook 和 ValidatingAdmissionWebhook 控制器。 这里是一组推荐的 admission 控制器，通常可以启用。 •确保启用了 admissionregistration.k8s.io/v1beta1 API。 配置准入 Webhook 你可以通过 ValidatingWebhookConfiguration 或者 MutatingWebhookConfiguration 动态配置哪些资源要被哪些准入 Webhook 处理。详细配置可以参阅 Webhook配置 部分。 认证和信任 默认情况下，apiserver不会向webhooks进行身份验证。但是，如果您想对客户端进行身份验证，可以将apiserver配置为使用基本身份验证、承载令牌或证书对Webhook进行身份验证。你可以在这里找到详细的步骤。 编写一个准入 Webhook 服务器 Webhook Admission 属于同步调用，需要用户部署自己的 webhook server，创建自定义的配置资源对象： ValidatingWebhookConfiguration 或 MutatingWebhookConfiguration。下面使用 kubebuilder 开发一个简单的 demo。 6.1 创建项目 1kubebuilder init --domain marklu.com --owner &quot;marklu&quot; --repo marklu.com/kubegame 提示： 这里通过 kubebuilder v3 创建的话，在 config 目录下会缺少 certmanager、webhook 目录以及 default/manager_webhook_patch.yml 和 webhookcainjection_patch.yaml 文件。可以通过从v2生成拷贝过来进行修改。 6.2 创建控制器 这里只需要创建一个控制器 1kubebuilder create api --group svc --version v1 --kind App 6.3 创建 webhook Implement Your Handler 新增 mutatingwebhook.go &amp; validatingwebhook.go 文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950// mutatingwebhook.gopackage controllersimport ( &quot;context&quot; &quot;encoding/json&quot; corev1 &quot;k8s.io/api/core/v1&quot; &quot;net/http&quot; &quot;sigs.k8s.io/controller-runtime/pkg/client&quot; &quot;sigs.k8s.io/controller-runtime/pkg/webhook/admission&quot;)// +kubebuilder:webhook:admissionReviewVersions=v1,sideEffects=None,path=/mutate-v1-svc,mutating=true,failurePolicy=fail,groups=&quot;&quot;,resources=services,verbs=create;update,versions=v1,name=msvc.kb.io// KubeGameAnnotator annotates Podstype KubeGameAnnotator struct &#123; Client client.Client decoder *admission.Decoder&#125;// Handle adds an annotation to every incoming pods.func (a *KubeGameAnnotator) Handle(ctx context.Context, req admission.Request) admission.Response &#123; pod := &amp;corev1.Pod&#123;&#125; err := a.decoder.Decode(req, pod) if err != nil &#123; return admission.Errored(http.StatusBadRequest, err) &#125; if pod.Annotations == nil &#123; pod.Annotations = map[string]string&#123;&#125; &#125; pod.Annotations[&quot;example-mutating-admission-webhook&quot;] = &quot;foo&quot; marshaledPod, err := json.Marshal(pod) if err != nil &#123; return admission.Errored(http.StatusInternalServerError, err) &#125; return admission.PatchResponseFromRaw(req.Object.Raw, marshaledPod)&#125;// KubeGameAnnotator implements admission.DecoderInjector.// A decoder will be automatically injected.// InjectDecoder injects the decoder.func (a *KubeGameAnnotator) InjectDecoder(d *admission.Decoder) error &#123; a.decoder = d return nil&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849// validatingwebhook.gopackage controllersimport ( &quot;context&quot; &quot;fmt&quot; corev1 &quot;k8s.io/api/core/v1&quot; &quot;net/http&quot; &quot;sigs.k8s.io/controller-runtime/pkg/client&quot; &quot;sigs.k8s.io/controller-runtime/pkg/webhook/admission&quot;)// +kubebuilder:webhook:admissionReviewVersions=v1,sideEffects=None,path=/validate-v1-svc,mutating=false,failurePolicy=fail,groups=&quot;&quot;,resources=services,verbs=create;update,versions=v1,name=vsvc.kb.io// KubeGameValidator validates Podstype KubeGameValidator struct &#123; Client client.Client decoder *admission.Decoder&#125;// Handle admits a pod if a specific annotation exists.func (v *KubeGameValidator) Handle(ctx context.Context, req admission.Request) admission.Response &#123; pod := &amp;corev1.Pod&#123;&#125; err := v.decoder.Decode(req, pod) if err != nil &#123; return admission.Errored(http.StatusBadRequest, err) &#125; key := &quot;example-mutating-admission-webhook&quot; anno, found := pod.Annotations[key] if !found &#123; return admission.Denied(fmt.Sprintf(&quot;missing annotation %s&quot;, key)) &#125; if anno != &quot;foo&quot; &#123; return admission.Denied(fmt.Sprintf(&quot;annotation %s did not have value %q&quot;, key, &quot;foo&quot;)) &#125; return admission.Allowed(&quot;&quot;)&#125;// KubeGameValidator implements admission.DecoderInjector.// A decoder will be automatically injected.// InjectDecoder injects the decoder.func (v *KubeGameValidator) InjectDecoder(d *admission.Decoder) error &#123; v.decoder = d return nil&#125; 注意：因为上述逻辑需要services权限，所以我们在控制器里需要添加如下内容 //+kubebuilder:rbac:groups=“”,resources=services,verbs=get;list;watch;create;update;patch;delete 用于生成 rbac manifests。 Register Your Handler 修改 main.go ，注册我们的 webhook handler 123456setupLog.Info(&quot;setting up webhook server&quot;)hookServer := mgr.GetWebhookServer()setupLog.Info(&quot;registering webhooks to the webhook server&quot;)hookServer.Register(&quot;/mutate-v1-svc&quot;, &amp;webhook.Admission&#123;Handler: &amp;controllers.KubeGameAnnotator&#123;Client: mgr.GetClient()&#125;&#125;)hookServer.Register(&quot;/validate-v1-svc&quot;, &amp;webhook.Admission&#123;Handler: &amp;controllers.KubeGameValidator&#123;Client: mgr.GetClient()&#125;&#125;) 提示： 这里注册的path（例如 validate-v1-sv）路径需要和 validatingwebhook.go 、mutatingwebhook.go 文件里的 CRD validation 匹配，不然 kustomize 生成出来的 webhook yaml 文件不对。 本地测试 make run 会报如下错误，是因为没有证书导致，需要配置证书，可以手动签发证书。 11.646924212701068e+09 ERROR setup problem running manager &#123;&quot;error&quot;: &quot;open /var/folders/67/375276sx6hv0nln1whwm5syh0000gq/T/k8s-webhook-server/serving-certs/tls.crt: no such file or directory&quot;&#125; 我本地指定证书目录： 123456789mgr, err := ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options&#123; Scheme: scheme, MetricsBindAddress: metricsAddr, Port: 9443, HealthProbeBindAddress: probeAddr, LeaderElection: enableLeaderElection, LeaderElectionID: &quot;27e1b0af.blazehu.com&quot;, CertDir: &quot;./cert/&quot;, &#125;) 重新启动发现恢复正常 提示： run controller-gen rbac:roleName=manager-role crd webhook paths=./… output:crd:artifacts:config=config/crd/bases -w to see all available markers, or controller-gen rbac:roleName=manager-role crd webhook paths=./… output:crd:artifacts:config=config/crd/bases -h for usage 7. 部署至集群 7.1 部署 cert manager 建议使用 certmanager 为 webhook 服务器提供证书。其他解决方案也有效，只要它们将证书放在所需的位置。安装文档点这里 通过如下方式注入 caBundle : 123456789101112131415# This patch add annotation to admission webhook config and# the variables $(CERTIFICATE_NAMESPACE) and $(CERTIFICATE_NAME) will be substituted by kustomize.apiVersion: admissionregistration.k8s.io/v1kind: MutatingWebhookConfigurationmetadata: name: mutating-webhook-configuration annotations: cert-manager.io/inject-ca-from: $(CERTIFICATE_NAMESPACE)/$(CERTIFICATE_NAME)---apiVersion: admissionregistration.k8s.io/v1kind: ValidatingWebhookConfigurationmetadata: name: validating-webhook-configuration annotations: cert-manager.io/inject-ca-from: $(CERTIFICATE_NAMESPACE)/$(CERTIFICATE_NAME) 7.2 构建镜像 •镜像替换：default/manager_auth_proxy_patch.yaml 文件中的 gcr.io/kubebuilder/kube-rbac-proxy:v0.8.0 （网络慢） •Dockerfile 去掉 go mod download，直接使用本地 vendor 构建 （网络慢） •Dockerfile 去掉 COPY api/ api/， 因为没有创建 Resource •去掉 main.go 文件中配置的证书路径 12make docker-build IMG=xxxxmake docker-push IMG=xxxx 7.3 修改模版，然后部署 •修改 config/default/kustomization.yaml ， 将 webhook、certmanager 相关的注释去掉。 •修改 config/crd/kustomization.yaml ，将 webhook、certmanager 相关的注释去掉。 •修改 config/default/kustomization.yaml ， 将 crd 相关的给注释掉。 1make deploy IMG=xxxx 部署成功： 查看控制器日志： 7.4 测试 简单创建一个 service，webhook 会注入一个注解，并进行验证。下图可以看到成功注入。 控制日志： 说明：查看 MutatingWebhookConfiguration 配置可以看到 caBundle 被注入其中了。 8. 总结 总结下 webhook Admission 的优势： •webhook 可动态扩展 Admission 能力，满足自定义客户的需求。 •不需要重启 API Server，可通过创建 webhook configuration 热加载 webhook admission。 Reference documentation •https://kubernetes.io/zh/docs/reference/access-authn-authz/extensible-admission-controllers •https://kubernetes.io/zh/docs/tasks/tls/managing-tls-in-a-cluster/ •https://book.kubebuilder.io/reference/admission-webhook.html •https://github.com/kubernetes-sigs/controller-runtime/tree/master/examples/builtins","categories":[{"name":"Kubebuilder","slug":"Kubebuilder","permalink":"https://marklinglon.github.io/categories/Kubebuilder/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://marklinglon.github.io/tags/k8s/"}]},{"title":"Alertmanager","slug":"alertmanager","date":"2018-05-20T16:00:00.000Z","updated":"2023-05-29T06:31:18.338Z","comments":true,"path":"2018/05/21/alertmanager/","link":"","permalink":"https://marklinglon.github.io/2018/05/21/alertmanager/","excerpt":"","text":"123456789101112131415161718192021222324252627282930313233343536373839404142434445464748global: smtp_smarthost: &quot;&quot; smtp_from: &quot;&quot; smtp_auth_username: &quot;&quot; smtp_auth_password: &quot;&quot;route: route: group_by: [&quot;alertname&quot;,&quot;severity&quot;] group_wait: 60s # 接收到告警后，多久开始发送, group_interval: 120s # 多个分组发送报警的间隔时间 repeat_interval: 21600s # 每个周期报警的间隔时间，如果中间相同分组的告警有变化，走上边两个规则,如果恢复了，就不属于重复告警 receiver: devops routes: - receiver: devops group_wait: 10s match: severity: critical - receiver: devops group_wait: 10s match: severity: error - receiver: devops group_wait: 10s match: severity: warningreceivers:- name: &#x27;devops&#x27; webhook_configs: - url: http://alertmanager-wechat:8001 send_resolved: falseinhibit_rules: - source_match: severity: &#x27;critical&#x27; target_match: severity: &#x27;critical&#x27; equal: [&#x27;alertname&#x27;] - source_match: severity: &#x27;error&#x27; target_match: severity: &#x27;error&#x27; equal: [&#x27;alertname&#x27;] - source_match: severity: &#x27;warning&#x27; target_match: severity: &#x27;warning&#x27; equal: [&#x27;alertname&#x27;]","categories":[{"name":"prometheux","slug":"prometheux","permalink":"https://marklinglon.github.io/categories/prometheux/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://marklinglon.github.io/tags/k8s/"}]},{"title":"keypass激活码","slug":"keypass","date":"2018-05-19T16:00:00.000Z","updated":"2023-05-26T05:52:47.019Z","comments":true,"path":"2018/05/20/keypass/","link":"","permalink":"https://marklinglon.github.io/2018/05/20/keypass/","excerpt":"","text":"激活码 12345678910111234-5678-abcd-efgh-jklm-nopq-rstu-vwxy-8765LWL1-5S5S-RT2S-23HT-9UPM-939M-HFSS-5GNP-FHF2RZ9Y-2TPK-EMHH-HUNE-HPK5-KM93-3EGK-RET9-RTN5LRQP-FKTM-ESEE-9R32-EE9G-SE3M-GRKG-2MFF-HMM3I5QU-5SNF-EE9S-FNRU-UUTS-MGPE-RR2G-SN5S-9EP2DFOH-PN29-RFFG-GS93-5EGT-TKKP-NS5H-F52M-URU3MDOQ-UPKM-K59F-SR3N-S3RP-MPS9-9T2E-2RG3-TSS3BOOU-NT3S-UR3R-U95G-M3GK-ERHM-3HG2-9SKG-S3N5M7DM-KMST-K22P-3229-9MG3-GK9S-PFK5-KMKU-SS55WKU4-H3PG-URNK-HS3S-PHK9-5R2N-MMKE-MTKR-HKE3ZJHL-REGF-HTUS-MH2R-PNSE-KHK5-5KM9-UP9K-NTG9","categories":[{"name":"keypass","slug":"keypass","permalink":"https://marklinglon.github.io/categories/keypass/"}],"tags":[{"name":"keypass","slug":"keypass","permalink":"https://marklinglon.github.io/tags/keypass/"}]}],"categories":[{"name":"gpt","slug":"gpt","permalink":"https://marklinglon.github.io/categories/gpt/"},{"name":"jumpserver","slug":"jumpserver","permalink":"https://marklinglon.github.io/categories/jumpserver/"},{"name":"k8s","slug":"k8s","permalink":"https://marklinglon.github.io/categories/k8s/"},{"name":"hexo","slug":"hexo","permalink":"https://marklinglon.github.io/categories/hexo/"},{"name":"Kubebuilder","slug":"Kubebuilder","permalink":"https://marklinglon.github.io/categories/Kubebuilder/"},{"name":"prometheux","slug":"prometheux","permalink":"https://marklinglon.github.io/categories/prometheux/"},{"name":"keypass","slug":"keypass","permalink":"https://marklinglon.github.io/categories/keypass/"}],"tags":[{"name":"gpt","slug":"gpt","permalink":"https://marklinglon.github.io/tags/gpt/"},{"name":"jumpserer","slug":"jumpserer","permalink":"https://marklinglon.github.io/tags/jumpserer/"},{"name":"k8s","slug":"k8s","permalink":"https://marklinglon.github.io/tags/k8s/"},{"name":"hexo","slug":"hexo","permalink":"https://marklinglon.github.io/tags/hexo/"},{"name":"keypass","slug":"keypass","permalink":"https://marklinglon.github.io/tags/keypass/"}]}